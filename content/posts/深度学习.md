---
title: "深度学习"
date: 2023-10-29T15:33:27+08:00
---

### 深度学习

#### 推荐书籍：

python深度学习：基于tensorflow 作者: 吴茂贵
https://weread.qq.com/web/bookDetail/07f320c07163ff6c07f8f02

python深度学习 2 作者: 弗朗索瓦·肖莱
https://weread.qq.com/web/bookDetail/33f32c90813ab71c6g018fff

在机器学习中，分类问题中的某个类别叫作类（class），数据点叫作样本（sample），与某个样本对应的类叫作标签（label）。

学习这里需要具备：

线性代数，概率论, 微积分等知识

一开始学习就把hello world问题看懂：MNIST问题MNIST

## 工作流程

首先，将训练数据（train_images和train_labels）输入神经网络；

然后，神经网络学习将图像和标签关联在一起；

最后，神经网络对test_images进行预测，我们来验证这些预测与test_labels中的标签是否匹配。

## 神经网络的数据表示

### 什么是张量

目前所有机器学习系统都使用张量作为基本数据结构。张量对这个领域非常重要，重要到TensorFlow都以它来命名。究竟什么是张量呢

张量这一概念的核心在于，它是一个数据容器。它包含的数据通常是数值数据，因此它是一个数字容器

张量是矩阵向任意维度的推广［注意，张量的维度通常叫作轴（axis）］

### 标量（0阶张量）

仅包含一个数字的张量叫作标量（scalar），也叫标量张量、0阶张量或0维张量

```shell
  >>> import numpy as np
  >>> x = np.array(12)
  >>> x
  array(12)
  >>> x.ndim
  0
```

### 向量（1阶张量）

数字组成的数组叫作向量（vector），也叫1阶张量或1维张量

```shell
  >>> x = np.array([12, 3, 6, 14, 7])
  >>> x
  array([12, 3, 6, 14, 7])
  >>> x.ndim
  1
```

这个向量包含5个元素，所以叫作5维向量。不要把5维向量和5维张量混为一谈

5维向量只有一个轴，沿着这个轴有5个维度，而5维张量有5个轴（沿着每个轴可能有任意个维度）

五维向量是指5个元素，指向一个方向(轴)

五维章量有5个轴，指向5个方向(轴)

### 矩阵（2阶张量）

向量组成的数组叫作矩阵（matrix），也叫2阶张量或2维张量。矩阵有2个轴（通常叫作行和列)

```shell
>>> x = np.array([[5, 78, 2, 34, 0],
                  [6, 79, 3, 35, 1],
                  [7, 80, 4, 36, 2]])
>>> x.ndim
2
```

第一个轴上的元素叫作行（row），第二个轴上的元素叫作列（column）。在上面的例子中，[5, 78, 2, 34, 0]是x的第一行，[5, 6, 7]是第一列。

### 3阶张量与更高阶的张量

将多个矩阵打包成一个新的数组，就可以得到一个3阶张量（或称为3维张量）

```shell
>>> x = np.array([[[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]]])
>>> x.ndim
3
```

将多个3阶张量打包成一个数组，就可以创建一个4阶张量，以此类推。深度学习处理的一般是0到4阶的张量，但处理视频数据时可能会遇到5阶张量。

### 张量关键属性

轴的个数（阶数）。举例来说，3阶张量有3个轴，矩阵有2个轴。这在NumPy或TensorFlow等Python库中也叫张量的ndim。

形状。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。举例来说，前面的矩阵示例的形状为(3, 5)，3阶张量示例的形状为(3, 3, 5)。向量的形状只包含一个元素，比如(5,)，而标量的形状为空，即()。

```py
标量
np.array(1) # 0个元素
# 向量
np.array([12, 3, 6, 14, 7]) # 5个元素
# 矩阵
np.array([[5, 78, 2, 34, 0],
                  [6, 79, 3, 35, 1],
                  [7, 80, 4, 36, 2]]) # (3, 5) 个元素
# 三阶张量
np.array([[[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]]]) # (3, 3, 5) 个元素
```

数据类型（在Python库中通常叫作dtype）。这是张量中所包含数据的类型。举例来说，张量的类型可以是float16、float32、float64、uint8等。在TensorFlow中，你还可能会遇到string类型的张量。

## 操作张量

选择张量的特定元素叫作张量切片

(60000, 28, 28)

由60000个矩阵组成的数组，每个矩阵由28×28个整数组成

train_images 是个三阶段张量

是一个图片

```shell
>>> train_images.shape
(60000, 28, 28)
>>> my_slice = train_images[10:100]
>>> my_slice.shape
(90, 28, 28)

>>> my_slice = train_images[10:100, :, :]  ←----等同于前面的例子
>>> my_slice.shape
(90, 28, 28)

>>> my_slice = train_images[10:100, 0:28, 0:28]  ←----也等同于前面的例子
>>> my_slice.shape
(90, 28, 28)
```

右下角选出14像素×14像素的区域

```shell
my_slice = train_images[:, 14:, 14:]
```

图像中心裁剪出14像素×14像素的区域

```shell
my_slice = train_images[:, 7:-7, 7:-7]
```

## 数据批量的概念

深度学习中所有数据张量的第一个轴（也就是轴0，因为索引从0开始）都是样本轴[samples axis，有时也叫样本维度（samples dimension）]。在MNIST例子中，样本就是数字图像。

此外，深度学习模型不会一次性处理整个数据集，而是将数据拆分成小批量。

## 基本流程

模型由许多层链接在一起组成，并将输入数据映射为预测值。随后，损失函数将这些预测值与目标值进行比较，得到一个损失值，用于衡量模型预测值与预期结果之间的匹配程度。优化器将利用这个损失值来更新模型权重

1. **模型结构**：神经网络由许多层链接在一起组成。这些层通常分为输入层、隐藏层和输出层。每个神经元在一层中与前一层的每个神经元相连，这种连接称为权重。这些权重是模型需要学习的参数，用来映射输入数据到预测值。

2. **前向传播**：训练的第一步是前向传播。在前向传播过程中，输入数据通过网络的每一层，经过权重的线性组合和激活函数的处理，最终产生模型的预测值。这个过程会一直持续，直到达到输出层。

3. **损失函数**：损失函数用于衡量模型的预测值与实际目标值之间的差距。通常，损失函数会计算这个差距的度量，比如均方误差（MSE）用于回归问题，或交叉熵用于分类问题。目标是最小化这个损失函数的值。

4. **反向传播**：一旦有了损失值，反向传播算法会被用来计算每个权重对损失的贡献，也就是梯度。这个梯度表示了如果稍微改变每个权重，损失值会如何改变。反向传播通过链式规则来计算这些梯度。

5. **优化器**：优化器的作用是根据梯度信息来更新神经网络的权重，以减小损失函数的值。常见的优化算法包括随机梯度下降（SGD）、Adam、RMSprop等。这些算法会根据梯度的方向和大小来更新权重，以使损失值逐渐减小。

6. **迭代训练**：这个过程是迭代的，模型会不断地进行前向传播、损失计算、反向传播和权重更新。这会一直持续，直到损失值收敛到一个满意的水平，或者达到预定的训练轮数。

通过这个过程，神经网络逐渐学习到如何根据输入数据进行预测，并且不断优化模型参数，以使预测与实际目标值更加吻合。

原始数据：
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

模型：
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])

编译：
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

训练循环：
model.fit(train_images, train_labels, epochs=5, batch_size=128)

